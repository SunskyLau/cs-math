# 非定常迭代法

## **第一步：线性代数基础复习**
### **1.1 向量与矩阵**
- **向量**：一组数的排列，如 $\mathbf{v} = [1, 2, 3]^T$。
- **矩阵**：数的二维排列，如 $A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$。
- **矩阵-向量乘法**：$A\mathbf{x}$ 表示对向量 $\mathbf{x}$ 的线性变换。

#### **例子：矩阵-向量乘法**
设 $A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$，$\mathbf{x} = [5, 6]^T$，则：
$$
A\mathbf{x} = \begin{bmatrix} 1 \cdot 5 + 2 \cdot 6 \\ 3 \cdot 5 + 4 \cdot 6 \end{bmatrix} = \begin{bmatrix} 17 \\ 39 \end{bmatrix}
$$

---

### **1.2 向量内积与正交性**
- **内积**：$\mathbf{u}^T \mathbf{v} = u_1v_1 + u_2v_2 + \dots + u_nv_n$。
- **正交**：若 $\mathbf{u}^T \mathbf{v} = 0$，则 $\mathbf{u}$ 和 $\mathbf{v}$ 正交。

#### **例子：正交向量**
$\mathbf{u} = [1, 0]^T$，$\mathbf{v} = [0, 1]^T$，则：
$$
\mathbf{u}^T \mathbf{v} = 1 \cdot 0 + 0 \cdot 1 = 0 \quad \text{（正交）}
$$

---

### **1.3 线性方程组**
目标：解 $A\mathbf{x} = \mathbf{b}$，即找到 $\mathbf{x}$ 使得矩阵 $A$ 与 $\mathbf{x}$ 的乘积等于 $\mathbf{b}$。

#### **例子：简单方程组**
设 $A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$，$\mathbf{b} = [3, 2]^T$，则解为：
$$
\mathbf{x} = \begin{bmatrix} 1 \\ 2 \end{bmatrix} \quad \text{（因为 } A\mathbf{x} = [1+2, 0+2]^T = [3, 2]^T \text{）}
$$

---

## **第二步：Krylov 子空间方法的核心思想**
### **2.1 投影方法（Projection Methods）**
- **目标**：将大规模问题 $A\mathbf{x} = \mathbf{b}$ 转化为低维子空间中的近似问题。
- **关键步骤**：
   1. 定义低维子空间 $K$（如 Krylov 子空间）。
   2. 在 $K$ 中寻找近似解 $\mathbf{x}_{\text{approx}}$，满足残量 $\mathbf{r} = \mathbf{b} - A\mathbf{x}_{\text{approx}}$ 与另一个子空间 $L$ 正交。

#### **例子：投影方法的几何直观**
假设原问题在三维空间中，我们选择二维平面 $K$ 作为子空间。投影方法的目标是：
- 在平面 $K$ 中找到一个点 $\mathbf{x}_{\text{approx}}$，使得残量 $\mathbf{r} = \mathbf{b} - A\mathbf{x}_{\text{approx}}$ 与某个约束空间 $L$ 正交（即垂直于 $L$ 中的所有向量）。

---

### **2.2 Krylov 子空间**
- **定义**：给定矩阵 $A$ 和初始残量 $\mathbf{r}_0$，Krylov 子空间为：
   $$
   K_m(A, \mathbf{r}_0) = \text{span}\{\mathbf{r}_0, A\mathbf{r}_0, A^2\mathbf{r}_0, \dots, A^{m-1}\mathbf{r}_0\}
   $$
- **特点**：嵌套性（$K_1 \subset K_2 \subset \dots$），适合迭代扩展。

#### **例子：构造 Krylov 子空间**
设 $A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$，初始残量 $\mathbf{r}_0 = [1, 0]^T$，则：
- $K_1 = \text{span}\{\mathbf{r}_0\}$
- $K_2 = \text{span}\{\mathbf{r}_0, A\mathbf{r}_0\} = \text{span}\left\{[1, 0]^T, [1, 3]^T\right\}$

---

## **第三步：Arnoldi 过程（构造正交基）**
### **3.1 Arnoldi 过程的目标**
**构造 Krylov 子空间 $\mathcal{K}_m(A, \mathbf{r}_0)$ 的一组标准正交基**

- **目标**：生成 Krylov 子空间 $\mathcal{K}_m(A, \mathbf{r}_0) = \text{span}\{\mathbf{r}_0, A\mathbf{r}_0, A^2\mathbf{r}_0, \dots, A^{m-1}\mathbf{r}_0\}$ 的一组标准正交基。
- **方法**：使用 **Arnoldi 过程** 来实现这一目标。

---

### **3.2 基于 MGS（Modified Gram-Schmidt）的 Arnoldi 过程**
#### **算法步骤详解：**
1. **初始化**：
   计算初始向量 $\mathbf{q}_1$：
   $$
   \mathbf{q}_1 = \frac{\mathbf{r}_0}{\|\mathbf{r}_0\|_2}
   $$
   其中 $\mathbf{r}_0$ 是初始残量，$\|\mathbf{r}_0\|_2$ 是其欧几里得范数。

2. **主循环**（对 $j = 1, 2, \dots, m-1$）：
   - **计算 $\mathbf{w}_j$**：
     $$
     \mathbf{w}_j = A\mathbf{q}_j
     $$
     即矩阵 $A$ 作用于当前基向量 $\mathbf{q}_j$。
   
   - **正交化**：
     对于每个已有的基向量 $\mathbf{q}_i$（从 $i = 1$ 到 $j$）：
     - 计算投影系数 $h_{ij}$：
       $$
       h_{ij} = \mathbf{q}_i^T \mathbf{w}_j
       $$
     - 从 $\mathbf{w}_j$ 中减去其在 $\mathbf{q}_i$ 上的投影：
       $$
       \mathbf{w}_j := \mathbf{w}_j - h_{ij} \mathbf{q}_i
       $$
   
   - **归一化**：
     - 计算 $\mathbf{w}_j$ 的范数：
       $$
       h_{j+1,j} = \|\mathbf{w}_j\|_2
       $$
     - 如果 $h_{j+1,j} = 0$，说明 $\mathbf{w}_j$ 已经线性相关，可以提前终止。
     - 否则，归一化得到新的正交基向量：
       $$
       \mathbf{q}_{j+1} = \frac{\mathbf{w}_j}{h_{j+1,j}}
       $$

3. **结束条件**：
   当 $j$ 达到 $m-1$ 或者出现线性相关（$h_{j+1,j} = 0$）时，算法结束。

---

### **3.3 输出**
Arnoldi 过程的输出包括：
- 一组标准正交基 $\{q_1, q_2, \dots, q_m\}$，它们张成 Krylov 子空间 $\mathcal{K}_m(A, v)$。
- 一个上 Hessenberg 矩阵 $H_m$，满足：
   $$
   AQ_m = Q_mH_m + h_{m+1,m}q_{m+1}e_m^T
   $$
   其中 $Q_m = [q_1, q_2, \dots, q_m]$ 是正交矩阵，$H_m$ 是上 Hessenberg 矩阵。

#### **例子：Arnoldi 过程**
设 $A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$，初始向量 $v = [1, 0]^T$：

1. **初始化**：
    - $q_1 = \frac{[1, 0]^T}{\|[1, 0]^T\|} = [1, 0]^T$

2. **第一步迭代**：
    - $Aq_1 = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} [1, 0]^T = [1, 3]^T$
    - $h_{11} = (Aq_1, q_1) = [1, 3]^T \cdot [1, 0]^T = 1$
    - $w_2 = Aq_1 - h_{11}q_1 = [1, 3]^T - 1 \cdot [1, 0]^T = [0, 3]^T$
    - $h_{21} = \|w_2\| = 3$
    - $q_2 = \frac{w_2}{h_{21}} = [0, 1]^T$

3. **结果**：
    - 正交基：$Q_2 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$
    - Hessenberg 矩阵：$H_2 = \begin{bmatrix} 1 & h_{12} \\ 3 & h_{22} \end{bmatrix}$

### **3.4 应用**
Arnoldi 过程是许多 Krylov 子空间方法的基础，例如：
- GMRES 方法利用 Arnoldi 过程构造 Krylov 子空间的正交基。
- 在特征值问题中，Arnoldi 过程可以用于求解大型稀疏矩阵的特征值和特征向量。

---

## **定理：Arnoldi 过程的基本关系**

### **定理内容**
设 $V_m = [v_1, v_2, \dots, v_m]$，则

$$
AV_m = V_{m+1}H_{m+1,m} = V_mH_m + h_{m+1,m}v_{m+1}e_m^T
$$

其中：
- $e_m = [0, \dots, 0, 1]^T \in \mathbb{R}^m$
- $H_m = H_{m+1,m}(1:m, 1:m) \in \mathbb{R}^{m \times m}$

### **定理解释**
- **前提**：$V_m$ 是由前 $m$ 个 Arnoldi 向量 $v_1, v_2, \dots, v_m$ 构成的矩阵。
- **结论**：
   1. $AV_m$ 可以分解为两部分：
       - $V_mH_m$：矩阵 $A$ 在 Krylov 子空间 $\mathcal{K}_m(A, r)$ 上的作用。
       - $h_{m+1,m}v_{m+1}e_m^T$：表示超出当前子空间的部分，由第 $m+1$ 个 Arnoldi 向量 $v_{m+1}$ 和系数 $h_{m+1,m}$ 给出。
   2. $V_m^TAV_m = H_m$：通过正交性，可以得到 $H_m$ 是一个上 Hessenberg 矩阵。

---

### **关键公式与矩阵结构**

#### **(1) $AV_m = V_{m+1}H_{m+1,m}$**
- **解释**：
   - $V_{m+1}$ 是包含前 $m+1$ 个 Arnoldi 向量的矩阵：
      $$
      V_{m+1} = [v_1, v_2, \dots, v_m, v_{m+1}]
      $$
   - $H_{m+1,m}$ 是一个 $(m+1) \times m$ 的上 Hessenberg 矩阵，其元素为投影系数 $h_{ij}$：
      $$
      H_{m+1,m} = \begin{bmatrix}
      h_{11} & h_{12} & \cdots & h_{1m} \\
      h_{21} & h_{22} & \cdots & h_{2m} \\
      0 & h_{32} & \cdots & h_{3m} \\
      \vdots & \ddots & \ddots & \vdots \\
      0 & \cdots & 0 & h_{mm} \\
      0 & \cdots & 0 & h_{m+1,m}
      \end{bmatrix}
      $$

#### **(2) $AV_m = V_mH_m + h_{m+1,m}v_{m+1}e_m^T$**
- **解释**：
   - $V_mH_m$ 表示矩阵 $A$ 在前 $m$ 个 Arnoldi 向量张成的子空间上的作用。
   - $h_{m+1,m}v_{m+1}e_m^T$ 表示超出当前子空间的部分，其中：
      - $e_m = [0, \dots, 0, 1]^T$ 是一个 $m$-维向量，只有最后一个分量为 1。
      - $v_{m+1}$ 是第 $m+1$ 个 Arnoldi 向量。
      - $h_{m+1,m}$ 是第 $m$ 步 Arnoldi 过程中的投影系数。

#### **(3) $V_m^TAV_m = H_m$**
- **解释**：
   - 由于 $V_m$ 的列是标准正交基，即 $V_m^TV_m = I_m$（单位矩阵）。
   - 因此，$V_m^TAV_m$ 是一个 $m \times m$ 的上 Hessenberg 矩阵 $H_m$，它是 $H_{m+1,m}$ 的左上角子矩阵：
      $$
      H_m = H_{m+1,m}(1:m, 1:m)
      $$

---

### **矩阵 $H_m$ 的结构**
根据上述公式，$H_m$ 是一个 $m \times m$ 的上 Hessenberg 矩阵，其形式为：
$$
H_m = \begin{bmatrix}
h_{11} & h_{12} & \cdots & h_{1,m-1} & h_{1,m} \\
h_{21} & h_{22} & \cdots & h_{2,m-1} & h_{2,m} \\
0 & h_{32} & \cdots & h_{3,m-1} & h_{3,m} \\
\vdots & \ddots & \ddots & \ddots & \vdots \\
0 & \cdots & 0 & h_{m,m-1} & h_{m,m}
\end{bmatrix}
$$

#### **特点**：
- **上 Hessenberg 结构**：除了主对角线和第一下三角带外，其余元素均为零。
- **投影系数**：矩阵中的元素 $h_{ij}$ 是 Arnoldi 过程中的投影系数。

---

### **定理意义**
这些公式揭示了矩阵 $A$ 在 Krylov 子空间上的行为，以及 Arnoldi 向量和 Hessenberg 矩阵之间的联系：
1. **子空间表示**：$AV_m$ 可以表示为当前子空间内的部分 $V_mH_m$ 和超出子空间的部分 $h_{m+1,m}v_{m+1}e_m^T$。
2. **投影关系**：$H_m$ 是矩阵 $A$ 在 Krylov 子空间上的投影表示。
3. **算法基础**：这些关系是 GMRES 等 Krylov 子空间方法的理论基础。


## **Krylov 子空间方法的基本框架**

### **基本算法流程**

#### **算法：Krylov 子空间方法的通用框架**

```plaintext
1: 选择初始向量 x^{(0)}
2: 计算 r_0 = b - Ax^{(0)}, v_1 = r_0 / ||r_0||_2
3: 寻找近似解：x^{(1)} ∈ x^{(0)} + K_1
4: if x^{(1)} 满足精度要求 then
5:     终止迭代
6: end if
7: for m = 2 to n
8:     调用 Arnoldi 过程计算向量 v_m
9:     寻找近似解：x^{(m)} ∈ x^{(0)} + K_m
10:    if x^{(m)} 满足精度要求 then
11:        终止迭代
12:    end if
13: end for
```

---

### **步骤详解**

#### **(1) 选择初始向量 $\mathbf{x}^{(0)}$**
- **目的**：提供一个初始猜测解
- **说明**：
   - 如果没有先验信息，可以选择 $\mathbf{x}^{(0)} = \mathbf{0}$（零向量）
   - 初始向量的选择会影响收敛速度，但通常不会影响最终解的准确性

#### **(2) 计算残量 $\mathbf{r}_0$ 和归一化向量 $\mathbf{v}_1$**
- **残量计算**：
   $$\mathbf{r}_0 = \mathbf{b} - A\mathbf{x}^{(0)}$$
   其中：
   - $\mathbf{b}$ 是右端向量
   - $A$ 是系数矩阵
   - $\mathbf{x}^{(0)}$ 是初始向量

- **归一化**：
   $$\mathbf{v}_1 = \frac{\mathbf{r}_0}{\|\mathbf{r}_0\|_2}$$
   即将残量 $\mathbf{r}_0$ 归一化为单位向量 $\mathbf{v}_1$，作为 Krylov 子空间的第一个基向量

#### **(3) 寻找近似解 $\mathbf{x}^{(1)}$**
- **目标**：在仿射空间 $\mathbf{x}^{(0)} + \mathcal{K}_1$ 中寻找近似解
- **解释**：
   - $\mathcal{K}_1 = \text{span}\{\mathbf{v}_1\}$ 是由第一个基向量 $\mathbf{v}_1$ 张成的一维子空间
   - 近似解的形式为：
      $$\mathbf{x}^{(1)} = \mathbf{x}^{(0)} + \alpha_1 \mathbf{v}_1$$
      其中 $\alpha_1$ 是待定系数

#### **(4) 检查精度要求**
- **条件**：如果当前近似解 $\mathbf{x}^{(1)}$ 满足精度要求，则终止迭代
- **精度判断**：
   - 通常通过残量范数来衡量：
      $$\|\mathbf{r}^{(1)}\| = \|\mathbf{b} - A\mathbf{x}^{(1)}\|$$
   - 如果 $\|\mathbf{r}^{(1)}\| \leq \text{tol}$（tol 是预设的容许误差），则认为满足精度要求

#### **(5) 调用 Arnoldi 过程扩展 Krylov 子空间**
- **目的**：逐步扩展 Krylov 子空间 $\mathcal{K}_m$ 的维度
- **过程**：
   - 对于 $m = 2, 3, \dots, n$：
      - 使用 Arnoldi 过程生成新的正交基向量 $\mathbf{v}_m$
      - 更新 Krylov 子空间：
         $$\mathcal{K}_m = \text{span}\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_m\}$$

#### **(6) 寻找近似解 $\mathbf{x}^{(m)}$**
- **目标**：在仿射空间 $\mathbf{x}^{(0)} + \mathcal{K}_m$ 中寻找近似解
- **形式**：
   $$\mathbf{x}^{(m)} = \mathbf{x}^{(0)} + \sum_{i=1}^m \alpha_i \mathbf{v}_i$$
   其中 $\alpha_i$ 是待定系数

#### **(7) 再次检查精度要求**
- **条件**：如果当前近似解 $\mathbf{x}^{(m)}$ 满足精度要求，则终止迭代
- **精度判断**：
   - 同样通过残量范数：
      $$\|\mathbf{r}^{(m)}\| = \|\mathbf{b} - A\mathbf{x}^{(m)}\|$$
   - 如果 $\|\mathbf{r}^{(m)}\| \leq \text{tol}$，则终止迭代

#### **(8) 循环结束**
如果在第 $m$ 步仍未达到精度要求，则继续扩展 Krylov 子空间，直到满足精度或达到最大迭代次数。

---

### **关键点总结**

#### **1. 初始阶段**
- 从初始向量 $\mathbf{x}^{(0)}$ 出发，计算残量 $\mathbf{r}_0$ 并归一化得到第一个基向量 $\mathbf{v}_1$
- 在 $\mathbf{x}^{(0)} + \mathcal{K}_1$ 中寻找近似解 $\mathbf{x}^{(1)}$

#### **2. 迭代扩展**
- 使用 Arnoldi 过程逐步扩展 Krylov 子空间 $\mathcal{K}_m$
- 在 $\mathbf{x}^{(0)} + \mathcal{K}_m$ 中寻找近似解 $\mathbf{x}^{(m)}$

#### **3. 精度检查**
- 每次迭代后检查残量范数 $\|\mathbf{r}^{(m)}\|$，判断是否满足精度要求
- 如果满足精度要求，则终止迭代；否则继续扩展子空间

#### **4. 核心工具**
- **Arnoldi 过程**：用于构造 Krylov 子空间的标准正交基
- **投影方法**：在 Krylov 子空间中寻找最优近似解

---

### **例子：基本框架的数值示例**

设 $A = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$，$\mathbf{b} = [3, 5]^T$，初始解 $\mathbf{x}^{(0)} = [0, 0]^T$：

1. **初始化**：
    - $\mathbf{r}_0 = \mathbf{b} - A\mathbf{x}^{(0)} = [3, 5]^T - [0, 0]^T = [3, 5]^T$
    - $\|\mathbf{r}_0\| = \sqrt{3^2 + 5^2} = \sqrt{34}$
    - $\mathbf{v}_1 = \frac{[3, 5]^T}{\sqrt{34}} = \left[\frac{3}{\sqrt{34}}, \frac{5}{\sqrt{34}}\right]^T$

2. **第一步近似解**：
    - 在 $\mathbf{x}^{(0)} + \mathcal{K}_1$ 中寻找 $\mathbf{x}^{(1)} = \mathbf{x}^{(0)} + \alpha_1 \mathbf{v}_1$
    - 通过最小化残量范数确定 $\alpha_1$

3. **扩展子空间**：
    - 如果 $\mathbf{x}^{(1)}$ 不满足精度要求，使用 Arnoldi 过程计算 $\mathbf{v}_2$
    - 在 $\mathbf{x}^{(0)} + \mathcal{K}_2$ 中寻找 $\mathbf{x}^{(2)}$

这个框架为 GMRES、CG 等具体方法提供了统一的理论基础。

## **完全正交方法 (FOM)**

### **1. FOM 的基本概念**

#### **1.1 定义**
**完全正交方法（Full Orthogonalization Method, FOM）** 是一种特殊的投影方法，其特点是：
$$
\mathcal{L}_m = \mathcal{K}_m
$$
即约束空间与搜索空间相同。

#### **1.2 投影方法的一般形式**
在一般的投影方法中，我们寻找近似解 $\tilde{\mathbf{x}}$ 满足：
$$
\text{find } \tilde{\mathbf{x}} \in \mathcal{K}_m \quad \text{such that} \quad \mathbf{b} - A\tilde{\mathbf{x}} \perp \mathcal{L}_m
$$
其中：
- $\mathcal{K}_m$ 是搜索空间（通常是 Krylov 子空间）
- $\mathcal{L}_m$ 是约束空间

#### **1.3 FOM 的特点**
在 FOM 中，选择 $\mathcal{L}_m = \mathcal{K}_m$，这意味着残量 $\mathbf{r} = \mathbf{b} - A\tilde{\mathbf{x}}$ 必须与整个 Krylov 子空间 $\mathcal{K}_m$ 正交。

---

### **2. FOM 的数学推导**

#### **2.1 近似解的形式**
设近似解为：
$$
\tilde{\mathbf{x}} = \mathbf{x}^{(0)} + V_m \tilde{\mathbf{y}}
$$
其中：
- $\mathbf{x}^{(0)}$ 是初始猜测解
- $V_m = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_m]$ 是由 Arnoldi 过程生成的 Krylov 子空间 $\mathcal{K}_m$ 的标准正交基矩阵
- $\tilde{\mathbf{y}} \in \mathbb{R}^m$ 是待求的系数向量

#### **2.2 正交性条件**
根据正交性条件 $\mathbf{b} - A\tilde{\mathbf{x}} \perp \mathcal{K}_m$，有：
$$
V_m^T (\mathbf{b} - A\tilde{\mathbf{x}}) = \mathbf{0}
$$

#### **2.3 代入近似解表达式**
将 $\tilde{\mathbf{x}} = \mathbf{x}^{(0)} + V_m \tilde{\mathbf{y}}$ 代入上述正交性条件：
$$
V_m^T (\mathbf{b} - A(\mathbf{x}^{(0)} + V_m \tilde{\mathbf{y}})) = \mathbf{0}
$$

展开后得到：
$$
V_m^T (\mathbf{b} - A\mathbf{x}^{(0)}) - V_m^T A V_m \tilde{\mathbf{y}} = \mathbf{0}
$$

#### **2.4 引入初始残量**
令初始残量为：
$$
\mathbf{r}_0 = \mathbf{b} - A\mathbf{x}^{(0)}
$$

则上式变为：
$$
V_m^T \mathbf{r}_0 - V_m^T A V_m \tilde{\mathbf{y}} = \mathbf{0}
$$

#### **2.5 利用 Arnoldi 关系**
根据 Arnoldi 过程的矩阵表示：
$$
A V_m = V_{m+1} H_{m+1,m}
$$

因此：
$$
V_m^T A V_m = V_m^T V_{m+1} H_{m+1,m}
$$

由于 $V_m$ 和 $V_{m+1}$ 的列是正交的，且 $V_m$ 的列是 $V_{m+1}$ 的前 $m$ 列，所以：
$$
V_m^T V_{m+1} = [I_m, \mathbf{0}]
$$

其中 $I_m$ 是 $m \times m$ 的单位矩阵。于是：
$$
V_m^T A V_m = H_m
$$

其中 $H_m$ 是 $H_{m+1,m}$ 的左上角 $m \times m$ 子矩阵（上 Hessenberg 矩阵）。

#### **2.6 最终方程**
将 $V_m^T A V_m = H_m$ 代入，得到：
$$
V_m^T \mathbf{r}_0 - H_m \tilde{\mathbf{y}} = \mathbf{0}
$$

进一步整理：
$$
H_m \tilde{\mathbf{y}} = V_m^T \mathbf{r}_0
$$

#### **2.7 初始残量的表示**
注意到在 Arnoldi 过程中，第一个基向量为：
$$
\mathbf{v}_1 = \frac{\mathbf{r}_0}{\|\mathbf{r}_0\|}
$$

因此：
$$
V_m^T \mathbf{r}_0 = \beta \mathbf{e}_1
$$

其中：
- $\beta = \|\mathbf{r}_0\|$ 是初始残量的范数
- $\mathbf{e}_1 = [1, 0, \ldots, 0]^T$ 是第一个标准单位向量

#### **2.8 线性方程组**
因此，FOM 归结为求解线性方程组：
$$
H_m \tilde{\mathbf{y}} = \beta \mathbf{e}_1
$$

#### **2.9 解出 $\tilde{\mathbf{y}}$**
如果 $H_m$ 非奇异，则可以求解：
$$
\tilde{\mathbf{y}} = H_m^{-1} \beta \mathbf{e}_1
$$

#### **2.10 近似解的最终表达式**
将 $\tilde{\mathbf{y}}$ 代入近似解公式：
$$
\tilde{\mathbf{x}} = \mathbf{x}^{(0)} + V_m \tilde{\mathbf{y}} = \mathbf{x}^{(0)} + \beta V_m H_m^{-1} \mathbf{e}_1
$$

---

### **3. FOM 算法流程**

#### **算法：完全正交方法 (FOM)**

```plaintext
1: 选择初始向量 x^{(0)}
2: 计算 r_0 = b - Ax^{(0)}, β = ||r_0||_2
3: v_1 = r_0 / β
4: 使用 Arnoldi 过程计算 V_m 和 H_m
5: 求解线性方程组 H_m y = β e_1
6: 计算近似解 x = x^{(0)} + V_m y
```

---

### **4. FOM 的性质与特点**

#### **4.1 优点**
1. **理论简洁**：基于简单的正交性条件
2. **精确解**：如果 $H_m$ 非奇异，可以得到在 Krylov 子空间中的精确投影

#### **4.2 缺点**
1. **数值不稳定**：当 $H_m$ 接近奇异时，数值稳定性差
2. **存储需求**：需要存储所有的 Arnoldi 向量 $V_m$
3. **计算复杂度**：需要求解线性方程组 $H_m \tilde{\mathbf{y}} = \beta \mathbf{e}_1$

#### **4.3 与 GMRES 的关系**
- **FOM**：要求残量与整个 Krylov 子空间正交
- **GMRES**：要求残量范数最小

GMRES 通常比 FOM 更稳定，因为它不需要 $H_m$ 非奇异的条件。

---

### **5. 数值例子**

#### **例子：FOM 方法求解**
设 $A = \begin{bmatrix} 2 & 1 \\ 0 & 3 \end{bmatrix}$，$\mathbf{b} = [3, 6]^T$，初始解 $\mathbf{x}^{(0)} = [0, 0]^T$：

1. **初始化**：
   - $\mathbf{r}_0 = \mathbf{b} - A\mathbf{x}^{(0)} = [3, 6]^T$
   - $\beta = \|\mathbf{r}_0\| = \sqrt{45} = 3\sqrt{5}$
   - $\mathbf{v}_1 = \frac{[3, 6]^T}{3\sqrt{5}} = \frac{1}{\sqrt{5}}[1, 2]^T$

2. **Arnoldi 过程**（$m=1$）：
   - $A\mathbf{v}_1 = \frac{1}{\sqrt{5}} \begin{bmatrix} 2 & 1 \\ 0 & 3 \end{bmatrix} [1, 2]^T = \frac{1}{\sqrt{5}}[4, 6]^T$
   - $h_{11} = \mathbf{v}_1^T A\mathbf{v}_1 = \frac{1}{5}[1, 2][4, 6]^T = \frac{16}{5}$
   - $H_1 = [h_{11}] = [\frac{16}{5}]$

3. **求解线性方程组**：
   - $H_1 \tilde{y} = \beta e_1$ 即 $\frac{16}{5} \tilde{y} = 3\sqrt{5}$
   - $\tilde{y} = \frac{15\sqrt{5}}{16}$

4. **计算近似解**：
   - $\tilde{\mathbf{x}} = \mathbf{x}^{(0)} + V_1 \tilde{y} = [0, 0]^T + \frac{15\sqrt{5}}{16} \cdot \frac{1}{\sqrt{5}}[1, 2]^T = \frac{15}{16}[1, 2]^T$

这个例子展示了 FOM 方法的具体计算过程。

## **第四步：GMRES 方法（非对称问题）**
### **4.1 GMRES 的核心思想**
**广义最小残量方法（Generalized Minimal Residual Method, GMRES）** 通过在 Krylov 子空间中寻找使残量范数最小的近似解来求解线性方程组 $A\mathbf{x} = \mathbf{b}$。

#### **4.1.1 最小化问题**
GMRES 的目标是解决以下最小化问题：
$$
\min_{\mathbf{y} \in \mathbb{R}^m} \|\mathbf{b} - A(\mathbf{x}^{(0)} + V_m\mathbf{y})\|_2
$$
其中：
- $\mathbf{x}^{(0)}$ 是初始猜测解
- $V_m = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_m]$ 是 Krylov 子空间的标准正交基
- $\mathbf{y} \in \mathbb{R}^m$ 是待求的系数向量

#### **4.1.2 与 FOM 的区别**
- **FOM**：要求残量与整个 Krylov 子空间正交（$\mathbf{r} \perp \mathcal{K}_m$）
- **GMRES**：要求残量范数最小（$\min \|\mathbf{r}\|_2$）

---

### **4.2 算法步骤详解**

#### **(1) 初始化**
1. **输入参数**：
   - 初始猜测解 $\mathbf{x}^{(0)}$
   - 相对精度要求 $\varepsilon > 0$

2. **计算初始残量和归一化**：
   $$
   \mathbf{r}_0 = \mathbf{b} - A\mathbf{x}^{(0)}, \quad \beta = \|\mathbf{r}_0\|_2, \quad \mathbf{v}_1 = \mathbf{r}_0 / \beta
   $$
   其中：
   - $\mathbf{r}_0$ 是初始残量
   - $\beta$ 是初始残量的范数
   - $\mathbf{v}_1$ 是归一化的初始残量，作为 Krylov 子空间的第一个基向量

#### **(2) 主循环（Arnoldi 过程）**
从第 $j = 1$ 开始，进入主循环，逐步扩展 Krylov 子空间并构造正交基。

##### **Step 1：计算 $\mathbf{w}_j = A\mathbf{v}_j$**
对于第 $j$ 步，计算矩阵 $A$ 作用于当前基向量 $\mathbf{v}_j$ 的结果：
$$
\mathbf{w}_j = A\mathbf{v}_j
$$

##### **Step 2：正交化过程（Modified Gram-Schmidt）**
使用 Modified Gram-Schmidt 方法对 $\mathbf{w}_j$ 进行正交化：
$$
\begin{align}
h_{ij} &= \mathbf{w}_j^T \mathbf{v}_i, \quad i = 1, 2, \ldots, j \\
\mathbf{w}_j &:= \mathbf{w}_j - h_{ij} \mathbf{v}_i
\end{align}
$$
其中：
- $h_{ij}$ 是投影系数，表示 $\mathbf{w}_j$ 在 $\mathbf{v}_i$ 上的投影
- 通过减去这些投影，确保 $\mathbf{w}_j$ 与前 $j$ 个基向量正交

##### **Step 3：计算 $h_{j+1,j}$**
计算正交化后 $\mathbf{w}_j$ 的范数：
$$
h_{j+1,j} = \|\mathbf{w}_j\|_2
$$

##### **Step 4：检查线性相关性**
如果 $h_{j+1,j} = 0$，说明 $\mathbf{w}_j$ 已经线性相关，无法继续生成新的正交基向量：
- 设置 $m = j$ 表示当前子空间维度为 $j$
- 退出 Arnoldi 循环

##### **Step 5：归一化得到 $\mathbf{v}_{j+1}$**
将 $\mathbf{w}_j$ 归一化，得到新的正交基向量：
$$
\mathbf{v}_{j+1} = \frac{\mathbf{w}_j}{h_{j+1,j}}
$$

##### **Step 6：检查精度**
计算当前残量的范数估计：
$$
\|\tilde{\mathbf{r}}\|_2 = \|\mathbf{b} - A(\mathbf{x}^{(0)} + V_j \mathbf{y})\|
$$
其中 $\mathbf{y}$ 是通过最小二乘问题求得的系数向量。

如果满足精度要求：
$$
\frac{\|\tilde{\mathbf{r}}\|_2}{\beta} < \varepsilon
$$
则设置 $m = j$ 并退出循环。

#### **(3) 求解最小二乘问题**
**核心思想**：利用 Arnoldi 关系将原问题转化为上 Hessenberg 线性方程组的最小二乘问题。

##### **Step 7：利用 Arnoldi 关系**
根据 Arnoldi 过程，有：
$$
AV_m = V_{m+1}H_{m+1,m}
$$

因此，最小化问题变为：
$$
\min_{\mathbf{y}} \|\mathbf{r}_0 - AV_m\mathbf{y}\|_2 = \min_{\mathbf{y}} \|\mathbf{r}_0 - V_{m+1}H_{m+1,m}\mathbf{y}\|_2
$$

##### **Step 8：利用正交性**
由于 $\mathbf{v}_1 = \mathbf{r}_0/\beta$，可以写成：
$$
\mathbf{r}_0 = \beta \mathbf{v}_1 = \beta V_{m+1}\mathbf{e}_1
$$

其中 $\mathbf{e}_1 = [1, 0, \ldots, 0]^T$ 是第一个标准单位向量。

##### **Step 9：转化为标准最小二乘问题**
最小化问题变为：
$$
\min_{\mathbf{y}} \|\beta V_{m+1}\mathbf{e}_1 - V_{m+1}H_{m+1,m}\mathbf{y}\|_2
$$

由于 $V_{m+1}$ 是正交矩阵，有 $\|V_{m+1}\mathbf{z}\|_2 = \|\mathbf{z}\|_2$，因此：
$$
\min_{\mathbf{y}} \|\beta \mathbf{e}_1 - H_{m+1,m}\mathbf{y}\|_2
$$

##### **Step 10：QR 分解求解最小二乘问题**

为了求解最小二乘问题 $\min_{\mathbf{y}} \|\beta \mathbf{e}_1 - H_{m+1,m}\mathbf{y}\|_2$，我们需要对上 Hessenberg 矩阵 $H_{m+1,m}$ 进行 QR 分解。

**QR 分解过程**：
对 $(m+1) \times m$ 的上 Hessenberg 矩阵 $H_{m+1,m}$ 进行 QR 分解：
$$
Q_m H_{m+1,m} = \begin{bmatrix} R_m \\ \mathbf{0}^T \end{bmatrix}
$$

其中：
- $Q_m$ 是 $(m+1) \times (m+1)$ 的正交矩阵（通过 Givens 旋转构造）
- $R_m$ 是 $m \times m$ 的上三角矩阵
- $\mathbf{0}^T$ 是零向量

**变换右端向量**：
同时对右端向量 $\beta \mathbf{e}_1$ 进行相同的变换：
$$
Q_m (\beta \mathbf{e}_1) = \begin{bmatrix} \beta \mathbf{q}_1(1:m) \\ \rho_m \end{bmatrix}
$$

其中：
- $\mathbf{q}_1(1:m)$ 是变换后向量的前 $m$ 个分量
- $\rho_m$ 是第 $m+1$ 个分量，其绝对值 $|\rho_m|$ 就是最小残量范数

**求解上三角线性方程组**：
最小二乘问题转化为求解上三角线性方程组：
$$
R_m \tilde{\mathbf{y}} = \beta \mathbf{q}_1(1:m)
$$

这个方程组可以通过回代法高效求解。

**残量范数的计算**：
最小残量范数为：
$$
\|\tilde{\mathbf{r}}\|_2 = |\rho_m|
$$

这使得我们可以在不计算实际近似解的情况下监控收敛性。

#### **(4) 计算近似解**
**Step 11：计算最终近似解**
$$
\tilde{\mathbf{x}} = \mathbf{x}^{(0)} + V_m \tilde{\mathbf{y}}
$$

其中：
- $V_m = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_m]$ 是 Krylov 子空间的标准正交基矩阵
- $\tilde{\mathbf{y}}$ 是最小二乘问题的解

---

### **4.3 GMRES 算法流程总结**

#### **算法：GMRES 方法**

```plaintext
1: 选择初始向量 x^{(0)}，设置容差 ε
2: 计算 r_0 = b - Ax^{(0)}, β = ||r_0||_2, v_1 = r_0/β
3: for j = 1, 2, ..., m
4:     w_j = Av_j
5:     for i = 1 to j
6:         h_{ij} = w_j^T v_i
7:         w_j = w_j - h_{ij} v_i
8:     end for
9:     h_{j+1,j} = ||w_j||_2
10:    if h_{j+1,j} = 0 then
11:        设置 m = j, 退出循环
12:    end if
13:    v_{j+1} = w_j / h_{j+1,j}
14:    求解最小二乘问题 min ||βe_1 - H_{j+1,j}y||_2
15:    if 相对残量 < ε then
16:        设置 m = j, 退出循环
17:    end if
18: end for
19: 求解 R_m y = β q_1(1:m)
20: 计算 x = x^{(0)} + V_m y
```

---

### **4.4 关键点总结**

#### **4.4.1 Arnoldi 过程**
- 用于逐步构造 Krylov 子空间的标准正交基 $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_m$
- 每一步计算 $A\mathbf{v}_j$，并通过正交化得到新的基向量 $\mathbf{v}_{j+1}$

#### **4.4.2 最小二乘问题**
- 通过求解上三角线性方程组 $R_m \tilde{\mathbf{y}} = \beta \mathbf{q}_1(1:m)$，找到使残量范数最小的系数向量 $\tilde{\mathbf{y}}$

#### **4.4.3 终止条件**
- **线性相关性**：如果 $h_{j+1,j} = 0$，提前终止
- **精度要求**：如果相对残量满足 $\|\tilde{\mathbf{r}}\|_2 / \beta < \varepsilon$，提前终止

#### **4.4.4 近似解的计算**
最终近似解为：
$$
\tilde{\mathbf{x}} = \mathbf{x}^{(0)} + V_m \tilde{\mathbf{y}}
$$

---

### **4.5 数值例子**

#### **例子：GMRES 求解详细步骤**

设 $A = \begin{bmatrix} 1 & 1 \\ 0 & 2 \end{bmatrix}$，$\mathbf{b} = [3, 4]^T$，初始解 $\mathbf{x}^{(0)} = [0, 0]^T$：

**步骤1：初始化**
- 初始残量：$\mathbf{r}_0 = \mathbf{b} - A\mathbf{x}^{(0)} = [3, 4]^T$
- 归一化：$\beta = \|\mathbf{r}_0\|_2 = 5$，$\mathbf{v}_1 = \mathbf{r}_0/\beta = [3/5, 4/5]^T$

**步骤2：Arnoldi过程（第1步）**
- 计算：$\mathbf{w}_1 = A\mathbf{v}_1 = \begin{bmatrix} 1 & 1 \\ 0 & 2 \end{bmatrix} \begin{bmatrix} 3/5 \\ 4/5 \end{bmatrix} = [7/5, 8/5]^T$
- 投影系数：$h_{11} = \mathbf{v}_1^T \mathbf{w}_1 = \frac{3 \cdot 7 + 4 \cdot 8}{25} = \frac{53}{25}$
- 正交化：$\mathbf{w}_1 := \mathbf{w}_1 - h_{11}\mathbf{v}_1 = [16/125, -12/125]^T$
- 归一化：$h_{21} = \|\mathbf{w}_1\|_2 = \frac{4}{25}$，$\mathbf{v}_2 = [4/5, -3/5]^T$

**步骤3：构造最小二乘问题（$m=1$）**
对于一维Krylov子空间，需要最小化：
$$\min_{y} \left\|\beta \mathbf{e}_1 - H_{2,1}y\right\|_2 = \min_{y} \left\|\begin{bmatrix} 5 \\ 0 \end{bmatrix} - y\begin{bmatrix} 53/25 \\ 4/25 \end{bmatrix}\right\|_2$$

根据最小二乘问题：
$$\min_y \|\beta e_1 - H_{2,1}y\|_2^2$$

展开得到：
$$\min_y \left\|\begin{bmatrix}\beta \\ 0\end{bmatrix} - \begin{bmatrix}h_{11} \\ h_{21}\end{bmatrix}y\right\|_2^2$$

对 $y$ 求导并令其等于零：
$$-2\begin{bmatrix}h_{11} & h_{21}\end{bmatrix}\left(\begin{bmatrix}\beta \\ 0\end{bmatrix} - \begin{bmatrix}h_{11} \\ h_{21}\end{bmatrix}y\right) = 0$$

解得最小二乘问题的解析解公式：
$$y = \frac{h_{11} \cdot \beta}{h_{11}^2 + h_{21}^2}$$

代入数值：
$$y = \frac{(53/25) \cdot 5}{(53/25)^2 + (4/25)^2} = \frac{265/25}{(53^2 + 16)/625} = \frac{265/25 \cdot 625}{53^2 + 16} = \frac{6625}{2825} \approx 2.34$$

**步骤4：计算近似解**
$$\tilde{\mathbf{x}} = \mathbf{x}^{(0)} + V_1 y = [0, 0]^T + [3/5, 4/5]^T \cdot 2.34 = [1.40, 1.87]^T$$

**验证：** 
残量 $\tilde{\mathbf{r}} = \mathbf{b} - A\tilde{\mathbf{x}} = [3, 4]^T - [3.27, 3.74]^T = [-0.27, 0.26]^T$
残量范数 $\|\tilde{\mathbf{r}}\|_2 \approx 0.37$

注：如需更高精度，可继续迭代到$m=2$。

#### **例子：GMRES 求解**
设 $A = \begin{bmatrix} 1 & 1 \\ 0 & 2 \end{bmatrix}$，$\mathbf{b} = [3, 4]^T$，初始解 $\mathbf{x}^{(0)} = [0, 0]^T$：
1. **初始残量**：
   - $\mathbf{r}_0 = \mathbf{b} - A\mathbf{x}^{(0)} = [3, 4]^T$
   - $\beta = \|\mathbf{r}_0\| = 5$, $\mathbf{v}_1 = [3/5, 4/5]^T$

2. **Arnoldi 构造**（假设 $m=1$）：
   - $V_1 = [\mathbf{v}_1]$
   - $H_{2,1} = [ \mathbf{v}_1^T A \mathbf{v}_1 ] = [ \text{计算得 } h_{11} ]$

3. **最小二乘问题**：
   - $\min_y \|5 \cdot [1, 0]^T - H_{2,1} y\|$
   - 解得 $y = 5/h_{11}$

4. **更新解**：
   - $\mathbf{x}^{(1)} = \mathbf{x}^{(0)} + V_1 y$

（注：实际计算需完整执行 Arnoldi 步骤，此处简化说明流程。）

---

## **第五步：共轭梯度法（CG，对称正定问题）**

### **5.1 为什么需要共轭梯度法？**
在数值计算中，大规模对称正定线性方程组指的是形如 $A\mathbf{x} = \mathbf{b}$ 的线性方程组，其中：

1. **大规模**：系数矩阵 $A$ 的维数很大（通常是成千上万或更多）
2. **对称性**：$A$ 满足 $A = A^T$（即 $a_{ij} = a_{ji}$）
3. **正定性**：对任意非零向量 $\mathbf{x}$，都有 $\mathbf{x}^T A \mathbf{x} > 0$

这类方程组经常出现在：
- 有限元分析
- 结构力学计算
- 图像处理
- 机器学习等领域

在求解这类方程组时，我们希望能找到一种既快速又节省存储的方法。共轭梯度法（Conjugate Gradient Method, CG）正是为此而生，它巧妙地利用了对称正定矩阵的特性，通过构造一组共轭方向来逐步逼近真实解。

### **5.2 核心思想**
CG 方法的核心是在 Krylov 子空间中寻找使能量泛函 $\|\mathbf{x} - \mathbf{x}^*\|_A$ 最小的解。这里的能量泛函实际上反映了当前解与真实解之间的"距离"。

### **5.3 算法推导**
为了最小化能量泛函，我们需要：
1. 构造一组搜索方向 $\{\mathbf{p}_k\}$
2. 在每个方向上找到最优步长 $\alpha_k$
3. 确保搜索方向之间满足共轭性质（即 $\mathbf{p}_i^T A \mathbf{p}_j = 0, i \neq j$）

### **5.4 CG 算法步骤**
1. **初始化**：
   - 选择初始解 $\mathbf{x}_0$（通常取零向量）
   - 计算初始残量 $\mathbf{r}_0 = \mathbf{b} - A\mathbf{x}_0$（表示当前解的误差）
   - 将初始搜索方向设为残量：$\mathbf{p}_0 = \mathbf{r}_0$

2. **迭代**（对 $k = 0, 1, \dots$）：
   - 计算最优步长：$\alpha_k = \frac{\mathbf{r}_k^T \mathbf{r}_k}{\mathbf{p}_k^T A \mathbf{p}_k}$（通过最小化能量泛函得到）
   - 更新解：$\mathbf{x}_{k+1} = \mathbf{x}_k + \alpha_k \mathbf{p}_k$（沿搜索方向前进）
   - 更新残量：$\mathbf{r}_{k+1} = \mathbf{r}_k - \alpha_k A \mathbf{p}_k$（计算新的误差）
   - 计算共轭系数：$\beta_k = \frac{\mathbf{r}_{k+1}^T \mathbf{r}_{k+1}}{\mathbf{r}_k^T \mathbf{r}_k}$（确保下一个搜索方向与之前的方向共轭）
   - 更新搜索方向：$\mathbf{p}_{k+1} = \mathbf{r}_{k+1} + \beta_k \mathbf{p}_k$（构造新的搜索方向）

### **5.5 具体示例**
让我们通过一个简单的例子来说明 CG 方法的工作过程。

设 $A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$（对称正定矩阵），$\mathbf{b} = [3, 4]^T$，初始解 $\mathbf{x}_0 = [0, 0]^T$：

1. **初始化阶段**：
   - 计算初始残量：$\mathbf{r}_0 = \mathbf{b} - A\mathbf{x}_0 = [3, 4]^T$
   - 设置初始搜索方向：$\mathbf{p}_0 = \mathbf{r}_0 = [3, 4]^T$

2. **第一次迭代**：
   - 计算步长：$\alpha_0 = \frac{\mathbf{r}_0^T \mathbf{r}_0}{\mathbf{p}_0^T A \mathbf{p}_0} = \frac{3^2 + 4^2}{[3, 4]A[3, 4]^T} = \frac{25}{61}$
   - 更新解：$\mathbf{x}_1 = \mathbf{x}_0 + \alpha_0 \mathbf{p}_0 = [75/61, 100/61]^T$
   - 计算新的残量：$\mathbf{r}_1 = \mathbf{b} - A\mathbf{x}_1$

每一步迭代都会使解更接近真实解，且由于对称正定性质，算法保证会在有限步内收敛到精确解。

---
## **第六步：重启策略（Restarted GMRES）**
### **6.1 动机**
避免存储和计算量随迭代步数 $m$ 增长。

### **6.2 重启流程**
1. 设定最大迭代步数 $m$，运行 GMRES 直到 $m$ 步。
2. 若未收敛，以当前解为新的初始解 $\mathbf{x}^{(0)}$，重新启动 GMRES。

#### **例子：重启 GMRES**
假设 $m=2$，每次迭代最多 2 步：
1. 第一次运行 GMRES，迭代 2 步得到近似解 $\mathbf{x}^{(2)}$。
2. 若残量不满足精度，令 $\mathbf{x}^{(0)} = \mathbf{x}^{(2)}$，重新运行 GMRES。

---

## **总结与考试策略**
1. **GMRES vs CG**：
   - 非对称问题用 GMRES，对称正定问题用 CG。
   - GMRES 需 Arnoldi 过程，CG 用三项递推公式。

2. **计算步骤记忆**：
   - Arnoldi：逐次正交化，生成 Hessenberg 矩阵。
   - GMRES：将问题转为最小二乘，用 Givens 旋转求解。
   - CG：记住 $\alpha_k, \beta_k$ 的计算公式。

3. **例题练习**：
   - 手动计算小规模（如 2x2）矩阵的 GMRES 或 CG 迭代步骤。
   - 理解残量范数如何随迭代减少。

4. **重启策略**：注意重启可能导致收敛变慢，但节省内存。

希望这份从零开始的详细讲解能帮你彻底掌握核心概念！如果需要更详细的例题或代码实现示例，请随时提问。