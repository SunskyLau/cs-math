# 向量和矩阵求导以及矩阵的迹（trace）

# 📌 一、基本概念

## 1. 变量类型与导数类型

| 导数类型 | 输入 | 输出 | 结果形式 |
|----------|------|------|-----------|
| 标量对标量 | 标量 $t$ | 标量 $f(t)$ | 标量 $\frac{df}{dt}$ |
| 向量对标量 | 标量 $t$ | 向量 $\mathbf{y}(t)$ | 向量 $\frac{d\mathbf{y}}{dt} \in \mathbb{R}^n$ |
| 标量对向量 | 向量 $\mathbf{x}$ | 标量 $f(\mathbf{x})$ | 向量（梯度）$\nabla_{\mathbf{x}} f \in \mathbb{R}^n$ |
| 向量对向量 | 向量 $\mathbf{x}$ | 向量 $\mathbf{y}(\mathbf{x})$ | 矩阵（雅可比）$\frac{\partial \mathbf{y}}{\partial \mathbf{x}} \in \mathbb{R}^{m \times n}$ |
| 标量对矩阵 | 矩阵 $\mathbf{X}$ | 标量 $f(\mathbf{X})$ | 矩阵 $\frac{\partial f}{\partial \mathbf{X}} \in \mathbb{R}^{m \times n}$ |
| 矩阵对矩阵 | 矩阵 $\mathbf{X}$ | 矩阵 $\mathbf{Y}(\mathbf{X})$ | 四维张量（不常用） |

---

# 📌 二、向量对标量的导数

设：
$$
\mathbf{y}(t) = 
\begin{bmatrix}
y_1(t) \\
y_2(t) \\
\vdots \\
y_n(t)
\end{bmatrix}
\in \mathbb{R}^n
$$
则：
$$
\frac{d\mathbf{y}}{dt} = 
\begin{bmatrix}
\frac{dy_1}{dt} \\
\frac{dy_2}{dt} \\
\vdots \\
\frac{dy_n}{dt}
\end{bmatrix}
\in \mathbb{R}^n
$$

### 示例：
$$
\mathbf{y}(t) = 
\begin{bmatrix}
t^2 \\
\sin t \\
e^t
\end{bmatrix}
\Rightarrow
\frac{d\mathbf{y}}{dt} =
\begin{bmatrix}
2t \\
\cos t \\
e^t
\end{bmatrix}
$$

---

# 📌 三、标量对向量的导数（梯度）

设 $f: \mathbb{R}^n \rightarrow \mathbb{R}$，定义为：
$$
\nabla_{\mathbf{x}} f = \frac{\partial f}{\partial \mathbf{x}} = 
\begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n}
\end{bmatrix}
\in \mathbb{R}^n
$$

### 示例：
$$
f(\mathbf{x}) = \mathbf{a}^\top \mathbf{x},\quad \frac{\partial f}{\partial \mathbf{x}} = \mathbf{a}
$$

---

# 📌 四、向量对向量的导数（雅可比矩阵）

设 $\mathbf{y} = f(\mathbf{x})$, 其中：
- $\mathbf{x} \in \mathbb{R}^n$
- $\mathbf{y} \in \mathbb{R}^m$

定义：
$$
\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = 
\begin{bmatrix}
\frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial y_m}{\partial x_1} & \cdots & \frac{\partial y_m}{\partial x_n}
\end{bmatrix}
\in \mathbb{R}^{m \times n}
$$

---

# 📌 五、标量对矩阵的导数

设 $f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}$，定义为：
$$
\frac{\partial f}{\partial \mathbf{X}} = 
\begin{bmatrix}
\frac{\partial f}{\partial x_{11}} & \cdots & \frac{\partial f}{\partial x_{1n}} \\
\vdots & \ddots & \vdots \\
\frac{\partial f}{\partial x_{m1}} & \cdots & \frac{\partial f}{\partial x_{mn}}
\end{bmatrix}
\in \mathbb{R}^{m \times n}
$$

---

# 📌 六、矩阵的迹（Trace）及其性质

### 定义：

对于方阵 $\mathbf{A} \in \mathbb{R}^{n \times n}$：
$$
\text{tr}(\mathbf{A}) = \sum_{i=1}^n a_{ii}
$$

### 常用性质：

| 性质 | 表达式 |
|------|--------|
| 线性性 | $\text{tr}(a\mathbf{A} + b\mathbf{B}) = a\,\text{tr}(\mathbf{A}) + b\,\text{tr}(\mathbf{B})$ |
| 转置不变性 | $\text{tr}(\mathbf{A}^\top) = \text{tr}(\mathbf{A})$ |
| 循环置换性 | $\text{tr}(\mathbf{A}\mathbf{B}\mathbf{C}) = \text{tr}(\mathbf{C}\mathbf{A}\mathbf{B}) = \text{tr}(\mathbf{B}\mathbf{C}\mathbf{A})$ |
| 内积表示 | $\text{tr}(\mathbf{A}^\top \mathbf{B}) = \sum_{i,j} A_{ij} B_{ij} = \langle \mathbf{A}, \mathbf{B} \rangle$ |

---

# 📌 七、常见的向量/矩阵求导公式（使用分母布局）

以下公式适用于 **标量函数对向量或矩阵求导**，是机器学习中最常用的公式。

---

## 🔹 向量相关公式

| 函数 | 导数 |
|------|------|
| $f = \mathbf{a}^\top \mathbf{x}$ | $\frac{\partial f}{\partial \mathbf{x}} = \mathbf{a}$ |
| $f = \mathbf{x}^\top \mathbf{A} \mathbf{x}$ | $\frac{\partial f}{\partial \mathbf{x}} = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}$ |
| $f = \mathbf{x}^\top \mathbf{x}$ | $\frac{\partial f}{\partial \mathbf{x}} = 2\mathbf{x}$ |
| $\mathbf{y} = \mathbf{A}\mathbf{x}$ | $\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \mathbf{A}^\top$ |
| $\mathbf{y} = \mathbf{x}^\top \mathbf{A}$ | $\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \mathbf{A}$ |

---

## 🔹 矩阵相关公式（标量对矩阵导数）

| 函数 | 导数 |
|------|------|
| $f = \text{tr}(\mathbf{A}\mathbf{X})$ | $\frac{\partial f}{\partial \mathbf{X}} = \mathbf{A}^\top$ |
| $f = \text{tr}(\mathbf{X}^\top \mathbf{A} \mathbf{X})$ | $\frac{\partial f}{\partial \mathbf{X}} = \mathbf{A}^\top \mathbf{X} + \mathbf{A} \mathbf{X}$ |
| $f = \text{tr}(\mathbf{X} \mathbf{A})$ | $\frac{\partial f}{\partial \mathbf{X}} = \mathbf{A}^\top$ |
| $f = \ln|\det(\mathbf{X})|$ | $\frac{\partial f}{\partial \mathbf{X}} = (\mathbf{X}^{-1})^\top$ |
| $f = \text{tr}(\mathbf{A} \mathbf{X} \mathbf{B})$ | $\frac{\partial f}{\partial \mathbf{X}} = \mathbf{A}^\top \mathbf{B}^\top$ |
| $f = \text{tr}(\mathbf{A} \mathbf{X}^\top \mathbf{B})$ | $\frac{\partial f}{\partial \mathbf{X}} = \mathbf{B} \mathbf{A}$ |

---

## 🔹 向量化操作（vec算子）与矩阵导数关系

若 $\mathbf{Y} = f(\mathbf{X})$，其中 $\mathbf{X}, \mathbf{Y} \in \mathbb{R}^{m \times n}$，则有：
$$
\frac{\partial \text{vec}(\mathbf{Y})}{\partial \text{vec}(\mathbf{X})}
\in \mathbb{R}^{mn \times mn}
$$

这是将矩阵映射转化为向量映射的一种方式。

---

# 📌 八、线性代数中的导数技巧示例

### 示例：最小二乘法中的梯度推导

目标函数：
$$
L = \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\top (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
$$

展开并求导：
$$
\frac{\partial L}{\partial \boldsymbol{\beta}} = -2\mathbf{X}^\top (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
$$

令导数为0，得正规方程：
$$
\mathbf{X}^\top \mathbf{X} \boldsymbol{\beta} = \mathbf{X}^\top \mathbf{y}
\Rightarrow \boldsymbol{\beta} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
$$